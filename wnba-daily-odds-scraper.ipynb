{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\n\nprint(\"This message prints immediately.\")\n\n# Pause execution for 3 seconds\ntime.sleep(45)\n\nprint(\"This message prints after a 3-second delay.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:56:52.861546Z","iopub.execute_input":"2025-08-25T05:56:52.862515Z","iopub.status.idle":"2025-08-25T05:56:55.868161Z","shell.execute_reply.started":"2025-08-25T05:56:52.862477Z","shell.execute_reply":"2025-08-25T05:56:55.867238Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# STEP 1: KAGGLE AUTH & PYTHON DEPENDENCIES\n# ==============================================================================\nprint(\"--- Installing Python Dependencies ---\")\n!pip install -q selenium pandas kaggle\n\nimport os\nimport pandas as pd\nimport logging\nimport json\nimport re\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nfrom importlib import reload\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException\n\n# Force logging to be active so we see all messages\nreload(logging)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nprint(\"\\n--- Setting up Kaggle API Authentication ---\")\napi = None\ntry:\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"KAGGLE_JSON\")\n    kaggle_dir = os.path.expanduser('~/.kaggle')\n    os.makedirs(kaggle_dir, exist_ok=True)\n    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n    with open(kaggle_json_path, 'w') as f: f.write(secret_value)\n    os.chmod(kaggle_json_path, 600)\n    \n    from kaggle.api.kaggle_api_extended import KaggleApi\n    api = KaggleApi()\n    api.authenticate()\n    print(\"Kaggle API Authentication Successful.\")\nexcept Exception as e:\n    logging.critical(f\"FATAL: A critical error occurred during Kaggle setup. Error: {e}\")\n    raise\n\n# ==============================================================================\n# STEP 2: SYSTEM INSTALLATIONS (CHROME)\n# ==============================================================================\nprint(\"\\n--- Installing Google Chrome & ChromeDriver ---\")\n# Using quiet flags to keep the log clean\n!sudo apt-get update > /dev/null\n!sudo apt-get install -y wget gnupg > /dev/null\n!wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -\n!sudo sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list'\n!sudo apt-get update > /dev/null\n!sudo apt-get install -y google-chrome-stable > /dev/null\n!apt-get install -y chromium-chromedriver > /dev/null\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin &>/dev/null\nprint(\"--- Chrome & ChromeDriver Setup Complete ---\")\n\n\n# ==============================================================================\n# STEP 3: SCRAPER FUNCTIONS (WITH ROBUSTNESS FIXES)\n# ==============================================================================\ndef get_all_leagues_and_games(driver):\n    \"\"\"\n    Scrapes the main basketball page with robust waits and debugging.\n    \"\"\"\n    url = \"https://www.pinnacle.com/en/basketball/matchups/\"\n    logging.info(f\"Navigating to matchups page: {url}\")\n    driver.get(url)\n\n    # Handle cookie banner if it appears\n    try:\n        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))).click()\n        logging.info(\"Clicked the Accept button for cookies.\"); time.sleep(2)\n    except TimeoutException:\n        logging.warning(\"Cookie banner not found or already handled.\")\n\n    leagues_data = {}\n    current_league_name = None\n\n    try:\n        content_container_selector = (By.CSS_SELECTOR, \".contentBlock.square\")\n        logging.info(\"Waiting for the main content container to load...\")\n        WebDriverWait(driver, 30).until(\n            EC.presence_of_element_located(content_container_selector)\n        )\n        logging.info(\"Main content container found. Proceeding to scrape rows.\")\n        \n        time.sleep(2)\n\n        all_rows = driver.find_elements(By.CSS_SELECTOR, \".contentBlock.square > div[class*='row-']\")\n        if not all_rows:\n            logging.error(\"Content container was found, but it contains no game or league rows.\")\n            with open(\"debug_page_no_rows.html\", \"w\", encoding=\"utf-8\") as f:\n                f.write(driver.page_source)\n            logging.info(\"Saved debug_page_no_rows.html to output for analysis.\")\n            return {}\n\n        logging.info(f\"Found {len(all_rows)} total rows to process on the matchups page.\")\n\n        for row in all_rows:\n            row_class = row.get_attribute('class')\n            \n            if 'row-CTcjEjV6yK' in row_class:\n                try:\n                    league_name = row.find_element(By.CSS_SELECTOR, \"a span\").text.strip()\n                    if league_name:\n                        current_league_name = league_name\n                        leagues_data[current_league_name] = []\n                        logging.info(f\"Discovered new league section: {current_league_name}\")\n                except NoSuchElementException:\n                    continue \n\n            elif 'row-k9ktBvvTsJ' in row_class and current_league_name:\n                try:\n                    game = {}\n                    link_tag = row.find_element(By.CSS_SELECTOR, \"a[href*='/basketball/']\")\n                    teams = link_tag.find_elements(By.CSS_SELECTOR, \"span.ellipsis.gameInfoLabel-EDDYv5xEfd\")\n                    game['team1'], game['team2'] = teams[0].text, teams[1].text\n                    game['game_link'] = link_tag.get_attribute('href')\n                    \n                    odds_groups = row.find_elements(By.CSS_SELECTOR, \"div.buttons-j19Jlcwsi9\")\n                    def get_text(elements, index): return elements[index].text if index < len(elements) else 'N/A'\n                    \n                    h_spans = odds_groups[0].find_elements(By.CSS_SELECTOR, \"button span\")\n                    ml_spans = odds_groups[1].find_elements(By.CSS_SELECTOR, \"span.price-r5BU0ynJha\")\n                    t_spans = odds_groups[2].find_elements(By.CSS_SELECTOR, \"button span\")\n                    \n                    game.update({'team1_moneyline': get_text(ml_spans, 0), 'team2_moneyline': get_text(ml_spans, 1),'team1_spread': get_text(h_spans, 0), 'team1_spread_odds': get_text(h_spans, 1),'team2_spread': get_text(h_spans, 2), 'team2_spread_odds': get_text(h_spans, 3),'over_total': get_text(t_spans, 0), 'over_total_odds': get_text(t_spans, 1),'under_total': get_text(t_spans, 2), 'under_total_odds': get_text(t_spans, 3)})\n                    \n                    leagues_data[current_league_name].append(game)\n                except (NoSuchElementException, IndexError):\n                    continue\n\n    except TimeoutException:\n        logging.error(\"FATAL: Timed out waiting for the main content container. The page may be blocked or changed.\")\n        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n            f.write(driver.page_source)\n        logging.info(\"Saved debug_page.html to output. This file will show what the scraper saw (e.g., a CAPTCHA).\")\n    \n    return leagues_data\n\ndef scrape_detailed_game_odds(driver, game_url):\n    logging.info(f\"Scraping detailed odds from: {game_url}\")\n    driver.get(game_url)\n    all_markets_data = []\n    try:\n        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.marketGroups-HjCkfKkLNt\"))); time.sleep(2)\n        market_groups = driver.find_elements(By.CSS_SELECTOR, \"div.marketGroup-wMlWprW2iC\")\n        for group in market_groups:\n            market_title = group.find_element(By.CSS_SELECTOR, \"span.titleText-BgvECQYfHf\").text\n            if not group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id]\"):\n                for btn in group.find_elements(By.CSS_SELECTOR, \"button\"):\n                    parts = btn.text.split('\\n')\n                    if len(parts) == 2: all_markets_data.append({'Market': market_title, 'Selection': parts[0], 'Odds': parts[1]})\n                continue\n            headers = [h.text for h in group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id] > li\")]\n            button_rows = group.find_elements(By.CSS_SELECTOR, \".buttonRow-zWMLOGu5YB\")\n            for row in button_rows:\n                buttons = row.find_elements(By.TAG_NAME, 'button')\n                if len(buttons) == len(headers):\n                    for i, btn in enumerate(buttons):\n                        parts = btn.text.split('\\n')\n                        if len(parts) == 2:\n                            selection_name = f\"{headers[i]} {parts[0]}\"\n                            all_markets_data.append({'Market': market_title, 'Selection': selection_name, 'Odds': parts[1]})\n    except TimeoutException:\n        logging.error(f\"Could not load market data for URL: {game_url}\")\n    return pd.DataFrame(all_markets_data)\n\ndef to_slug(name):\n    return re.sub(r'[^a-z0-9]+', '_', name.lower()).strip('_')\n\n# ==============================================================================\n# STEP 4: MAIN DATA PIPELINE EXECUTION\n# ==============================================================================\nprint(\"\\n--- Starting Data Pipeline Execution ---\")\nif __name__ == \"__main__\" and api:\n    DATASET_SLUG = \"zachht/wnba-odds-history\" \n    WORKING_DIR = \"/kaggle/working\"\n    \n    # --- NEW: SAFE INITIALIZATION STEP ---\n    # Before doing anything, download all existing files from the dataset.\n    # This ensures that we have a complete local copy. The script will then\n    # append to these files. If this step fails, the script will likely\n    # fail before the final upload, preventing accidental overwrites.\n    try:\n        print(f\"\\n--- Synchronizing with existing Kaggle dataset: {DATASET_SLUG} ---\")\n        api.dataset_download_files(DATASET_SLUG, path=WORKING_DIR, unzip=True)\n        print(\"Synchronization complete. Existing files are now in the working directory.\")\n    except Exception as e:\n        logging.critical(f\"FATAL ERROR: Could not download existing dataset. Aborting immediately to prevent data overwrite. Error: {e}\")\n        # This forces the script to exit. It will NOT continue to the scraper.\n        raise SystemExit(\"Script aborted: Failed to sync with existing dataset.\")\n\n    driver = None\n    leagues_updated = []\n    try:\n        logging.info(\"Initializing Selenium driver...\")\n        options = webdriver.ChromeOptions()\n        options.add_argument(\"--headless=new\") \n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--window-size=1920,1080\")\n        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\")\n        options.add_argument('--disable-blink-features=AutomationControlled')\n        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n        options.add_experimental_option('useAutomationExtension', False)\n        driver = webdriver.Chrome(options=options)\n        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n            'source': '''\n                Object.defineProperty(navigator, 'webdriver', {\n                  get: () => undefined\n                })\n            '''\n        })\n        logging.info(\"Selenium driver initialized.\")\n        \n        all_leagues_games = get_all_leagues_and_games(driver)\n\n        if not all_leagues_games:\n            logging.warning(\"Scraping finished, but no leagues were found on the site. Check debug files if they were created.\")\n        else:\n            for league_name, new_main_lines_data in all_leagues_games.items():\n                if not new_main_lines_data:\n                    logging.info(f\"No games found for league: {league_name}. Skipping.\")\n                    continue\n\n                logging.info(f\"\\n--- Processing League: {league_name} ({len(new_main_lines_data)} games found) ---\")\n                leagues_updated.append(league_name)\n                league_slug = to_slug(league_name)\n\n                MAIN_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_main_lines.csv\")\n                DETAILED_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_detailed_odds.csv\")\n\n                # --- MODIFIED: SAFE FILE LOADING ---\n                # Instead of downloading, we now read from the local directory.\n                # If the file doesn't exist (from the initial sync), we create a new DataFrame.\n                try:\n                    old_main_df = pd.read_csv(MAIN_CSV_PATH)\n                    logging.info(f\"Successfully loaded existing data from {os.path.basename(MAIN_CSV_PATH)}\")\n                except FileNotFoundError:\n                    logging.warning(f\"No existing file found for '{league_name}'. A new history file will be created.\")\n                    old_main_df = pd.DataFrame()\n                \n                try:\n                    old_detailed_df = pd.read_csv(DETAILED_CSV_PATH)\n                    logging.info(f\"Successfully loaded existing data from {os.path.basename(DETAILED_CSV_PATH)}\")\n                except FileNotFoundError:\n                    logging.warning(f\"No existing detailed odds file for '{league_name}'. A new file will be created.\")\n                    old_detailed_df = pd.DataFrame()\n\n\n                scrape_timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n                new_main_df = pd.DataFrame(new_main_lines_data)\n                new_main_df['timestamp'] = scrape_timestamp\n                combined_main_df = pd.concat([old_main_df, new_main_df], ignore_index=True)\n                \n                all_detailed_dfs = []\n                for game in new_main_lines_data:\n                    detailed_df = scrape_detailed_game_odds(driver, game['game_link'])\n                    if not detailed_df.empty:\n                        detailed_df['matchup'] = f\"{game['team1']} vs {game['team2']}\"\n                        all_detailed_dfs.append(detailed_df)\n                \n                if all_detailed_dfs:\n                    new_detailed_df = pd.concat(all_detailed_dfs, ignore_index=True)\n                    new_detailed_df['timestamp'] = scrape_timestamp\n                    combined_detailed_df = pd.concat([old_detailed_df, new_detailed_df], ignore_index=True)\n                    \n                    logging.info(f\"Saving combined data to local CSVs for {league_name}...\")\n                    combined_main_df.to_csv(MAIN_CSV_PATH, index=False)\n                    combined_detailed_df.to_csv(DETAILED_CSV_PATH, index=False)\n                else: # Still save the main lines even if detailed scraping fails\n                    logging.info(f\"Saving main lines data to local CSV for {league_name}...\")\n                    combined_main_df.to_csv(MAIN_CSV_PATH, index=False)\n\n            \n            if leagues_updated:\n                logging.info(\"\\n--- Finalizing and Uploading to Kaggle ---\")\n                metadata_path = os.path.join(WORKING_DIR, 'dataset-metadata.json')\n                metadata = {\"title\": \"Pinnacle Basketball Odds History\", \"id\": DATASET_SLUG, \"licenses\": [{\"name\": \"CC0-1.0\"}]}\n                with open(metadata_path, 'w') as f: json.dump(metadata, f)\n                \n                version_note = f\"Automated odds update. Leagues updated: {', '.join(leagues_updated)}.\"\n                logging.info(f\"Pushing new dataset version. {version_note}\")\n                # This command uploads the ENTIRE content of WORKING_DIR.\n                # Because we downloaded all files at the start, this is now safe.\n                # Any untouched files are re-uploaded, and updated ones are replaced.\n                api.dataset_create_version(folder=WORKING_DIR, version_notes=version_note, quiet=False, dir_mode='zip')\n            else:\n                logging.warning(\"No games were found for any leagues. No new version will be pushed.\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during the main pipeline: {e}\", exc_info=True)\n    finally:\n        if driver: driver.quit(); logging.info(\"Selenium driver closed.\")\n\nprint(\"\\n--- Data Pipeline Execution Finished ---\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:53:11.942052Z","iopub.execute_input":"2025-08-25T05:53:11.942475Z","iopub.status.idle":"2025-08-25T05:55:20.831304Z","shell.execute_reply.started":"2025-08-25T05:53:11.942446Z","shell.execute_reply":"2025-08-25T05:55:20.830273Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# STEP 1: KAGGLE AUTH & PYTHON DEPENDENCIES\n# ==============================================================================\nprint(\"--- Installing Python Dependencies ---\")\n!pip install -q selenium pandas kaggle undetected-chromedriver\n\nimport os\nimport pandas as pd\nimport logging\nimport json\nimport re\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nfrom importlib import reload\n\n# Force logging to be active so we see all messages\nreload(logging)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nprint(\"\\n--- Setting up Kaggle API Authentication ---\")\napi = None\ntry:\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"KAGGLE_JSON\")\n    kaggle_dir = os.path.expanduser('~/.kaggle')\n    os.makedirs(kaggle_dir, exist_ok=True)\n    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n    with open(kaggle_json_path, 'w') as f: f.write(secret_value)\n    os.chmod(kaggle_json_path, 600)\n    \n    from kaggle.api.kaggle_api_extended import KaggleApi\n    api = KaggleApi()\n    api.authenticate()\n    print(\"Kaggle API Authentication Successful.\")\nexcept Exception as e:\n    logging.critical(f\"FATAL: A critical error occurred during Kaggle setup. Error: {e}\")\n    raise\n\n# ==============================================================================\n# STEP 2: SYSTEM INSTALLATIONS (CHROME) - THIS IS INTENTIONALLY LEFT BLANK\n# ==============================================================================\nprint(\"\\n--- Skipping manual Chrome installation to use the environment's default ---\")\n\n\n# ==============================================================================\n# STEP 3: SCRAPER FUNCTIONS (WITH ROBUSTNESS FIXES)\n# ==============================================================================\nimport time\nimport undetected_chromedriver as uc\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException\n\ndef get_all_leagues_and_games(driver):\n    \"\"\"\n    Scrapes the main basketball page with robust waits and debugging.\n    \"\"\"\n    url = \"https://www.pinnacle.com/en/basketball/matchups/\"\n    logging.info(f\"Navigating to matchups page: {url}\")\n    driver.get(url)\n\n    # Handle cookie banner if it appears\n    try:\n        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))).click()\n        logging.info(\"Clicked the Accept button for cookies.\"); time.sleep(2)\n    except TimeoutException:\n        logging.warning(\"Cookie banner not found or already handled.\")\n\n    leagues_data = {}\n    current_league_name = None\n\n    try:\n        # Wait for the main content container to be present.\n        content_container_selector = (By.CSS_SELECTOR, \".contentBlock.square\")\n        logging.info(\"Waiting for the main content container to load...\")\n        WebDriverWait(driver, 30).until(\n            EC.presence_of_element_located(content_container_selector)\n        )\n        logging.info(\"Main content container found. Proceeding to scrape rows.\")\n        \n        time.sleep(2) # Give JS a moment to render after container is found\n\n        all_rows = driver.find_elements(By.CSS_SELECTOR, \".contentBlock.square > div[class*='row-']\")\n        if not all_rows:\n            logging.error(\"Content container was found, but it contains no game or league rows.\")\n            with open(\"debug_page_no_rows.html\", \"w\", encoding=\"utf-8\") as f:\n                f.write(driver.page_source)\n            logging.info(\"Saved debug_page_no_rows.html to output for analysis.\")\n            return {}\n\n        logging.info(f\"Found {len(all_rows)} total rows to process on the matchups page.\")\n\n        for row in all_rows:\n            row_class = row.get_attribute('class')\n            \n            if 'row-CTcjEjV6yK' in row_class:\n                try:\n                    league_name = row.find_element(By.CSS_SELECTOR, \"a span\").text.strip()\n                    if league_name:\n                        current_league_name = league_name\n                        leagues_data[current_league_name] = []\n                        logging.info(f\"Discovered new league section: {current_league_name}\")\n                except NoSuchElementException:\n                    continue \n\n            elif 'row-k9ktBvvTsJ' in row_class and current_league_name:\n                try:\n                    game = {}\n                    link_tag = row.find_element(By.CSS_SELECTOR, \"a[href*='/basketball/']\")\n                    teams = link_tag.find_elements(By.CSS_SELECTOR, \"span.ellipsis.gameInfoLabel-EDDYv5xEfd\")\n                    game['team1'], game['team2'] = teams[0].text, teams[1].text\n                    game['game_link'] = link_tag.get_attribute('href')\n                    \n                    odds_groups = row.find_elements(By.CSS_SELECTOR, \"div.buttons-j19Jlcwsi9\")\n                    def get_text(elements, index): return elements[index].text if index < len(elements) else 'N/A'\n                    \n                    h_spans = odds_groups[0].find_elements(By.CSS_SELECTOR, \"button span\")\n                    ml_spans = odds_groups[1].find_elements(By.CSS_SELECTOR, \"span.price-r5BU0ynJha\")\n                    t_spans = odds_groups[2].find_elements(By.CSS_SELECTOR, \"button span\")\n                    \n                    game.update({'team1_moneyline': get_text(ml_spans, 0), 'team2_moneyline': get_text(ml_spans, 1),'team1_spread': get_text(h_spans, 0), 'team1_spread_odds': get_text(h_spans, 1),'team2_spread': get_text(h_spans, 2), 'team2_spread_odds': get_text(h_spans, 3),'over_total': get_text(t_spans, 0), 'over_total_odds': get_text(t_spans, 1),'under_total': get_text(t_spans, 2), 'under_total_odds': get_text(t_spans, 3)})\n                    \n                    leagues_data[current_league_name].append(game)\n                except (NoSuchElementException, IndexError):\n                    continue\n\n    except TimeoutException:\n        logging.error(\"FATAL: Timed out waiting for the main content container. The page may be blocked or changed.\")\n        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n            f.write(driver.page_source)\n        logging.info(\"Saved debug_page.html to output. This file will show what the scraper saw (e.g., a CAPTCHA).\")\n    \n    return leagues_data\n\ndef scrape_detailed_game_odds(driver, game_url):\n    logging.info(f\"Scraping detailed odds from: {game_url}\")\n    driver.get(game_url)\n    all_markets_data = []\n    try:\n        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.marketGroups-HjCkfKkLNt\"))); time.sleep(2)\n        market_groups = driver.find_elements(By.CSS_SELECTOR, \"div.marketGroup-wMlWprW2iC\")\n        for group in market_groups:\n            market_title = group.find_element(By.CSS_SELECTOR, \"span.titleText-BgvECQYfHf\").text\n            if not group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id]\"):\n                for btn in group.find_elements(By.CSS_SELECTOR, \"button\"):\n                    parts = btn.text.split('\\n')\n                    if len(parts) == 2: all_markets_data.append({'Market': market_title, 'Selection': parts[0], 'Odds': parts[1]})\n                continue\n            headers = [h.text for h in group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id] > li\")]\n            button_rows = group.find_elements(By.CSS_SELECTOR, \".buttonRow-zWMLOGu5YB\")\n            for row in button_rows:\n                buttons = row.find_elements(By.TAG_NAME, 'button')\n                if len(buttons) == len(headers):\n                    for i, btn in enumerate(buttons):\n                        parts = btn.text.split('\\n')\n                        if len(parts) == 2:\n                            selection_name = f\"{headers[i]} {parts[0]}\"\n                            all_markets_data.append({'Market': market_title, 'Selection': selection_name, 'Odds': parts[1]})\n    except TimeoutException:\n        logging.error(f\"Could not load market data for URL: {game_url}\")\n    return pd.DataFrame(all_markets_data)\n\ndef to_slug(name):\n    return re.sub(r'[^a-z0-9]+', '_', name.lower()).strip('_')\n\n# ==============================================================================\n# STEP 4: MAIN DATA PIPELINE EXECUTION\n# ==============================================================================\nprint(\"\\n--- Starting Data Pipeline Execution ---\")\nif __name__ == \"__main__\" and api:\n    DATASET_SLUG = \"zachht/wnba-odds-history\" \n    WORKING_DIR = \"/kaggle/working\"\n    \n    # --- NEW: SAFE INITIALIZATION STEP ---\n    # Before scraping, download all existing files from the dataset to ensure\n    # the working directory is a complete mirror. This prevents accidental\n    # deletion of files that are not part of today's scrape.\n    try:\n        print(f\"\\n--- Synchronizing with existing Kaggle dataset: {DATASET_SLUG} ---\")\n        api.dataset_download_files(DATASET_SLUG, path=WORKING_DIR, unzip=True)\n        print(\"Synchronization complete. Existing files are now in the working directory.\")\n    except Exception as e:\n        logging.critical(f\"FATAL ERROR: Could not download existing dataset. Aborting immediately to prevent data overwrite. Error: {e}\")\n        # This forces the script to exit. It will NOT continue to the scraper.\n        raise SystemExit(\"Script aborted: Failed to sync with existing dataset.\")\n\n    driver = None\n    leagues_updated = []\n    try:\n        # CORRECT INITIALIZATION FOR UNDETECTED CHROMEDRIVER IN KAGGLE\n        logging.info(\"Initializing Undetected ChromeDriver...\")\n        options = uc.ChromeOptions()\n        options.add_argument(\"--headless\")\n        options.add_argument(\"--no-sandbox\")\n        options.add_argument('--disable-dev-shm-usage')\n        \n        driver = uc.Chrome(options=options)\n        logging.info(\"Undetected ChromeDriver initialized successfully.\")\n        \n        all_leagues_games = get_all_leagues_and_games(driver)\n\n        if not all_leagues_games:\n            logging.warning(\"Scraping finished, but no leagues were found on the site. Check debug files if they were created.\")\n        else:\n            for league_name, new_main_lines_data in all_leagues_games.items():\n                if not new_main_lines_data:\n                    logging.info(f\"No games found for league: {league_name}. Skipping.\")\n                    continue\n\n                logging.info(f\"\\n--- Processing League: {league_name} ({len(new_main_lines_data)} games found) ---\")\n                leagues_updated.append(league_name)\n                league_slug = to_slug(league_name)\n\n                MAIN_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_main_lines.csv\")\n                DETAILED_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_detailed_odds.csv\")\n\n                # --- MODIFIED: SAFE LOCAL FILE LOADING ---\n                # Read from the local directory. If a file doesn't exist (because it's\n                # a new league), create an empty DataFrame. This avoids risky downloads\n                # inside the loop.\n                try:\n                    old_main_df = pd.read_csv(MAIN_CSV_PATH)\n                    logging.info(f\"Loaded existing local data for {os.path.basename(MAIN_CSV_PATH)}\")\n                except FileNotFoundError:\n                    logging.warning(f\"No local file for '{league_name}' main lines. Creating a new one.\")\n                    old_main_df = pd.DataFrame()\n                \n                try:\n                    old_detailed_df = pd.read_csv(DETAILED_CSV_PATH)\n                    logging.info(f\"Loaded existing local data for {os.path.basename(DETAILED_CSV_PATH)}\")\n                except FileNotFoundError:\n                    logging.warning(f\"No local file for '{league_name}' detailed odds. Creating a new one.\")\n                    old_detailed_df = pd.DataFrame()\n\n                scrape_timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n                new_main_df = pd.DataFrame(new_main_lines_data)\n                new_main_df['timestamp'] = scrape_timestamp\n                combined_main_df = pd.concat([old_main_df, new_main_df], ignore_index=True)\n                \n                all_detailed_dfs = []\n                for game in new_main_lines_data:\n                    detailed_df = scrape_detailed_game_odds(driver, game['game_link'])\n                    if not detailed_df.empty:\n                        detailed_df['matchup'] = f\"{game['team1']} vs {game['team2']}\"\n                        all_detailed_dfs.append(detailed_df)\n                \n                if all_detailed_dfs:\n                    new_detailed_df = pd.concat(all_detailed_dfs, ignore_index=True)\n                    new_detailed_df['timestamp'] = scrape_timestamp\n                    combined_detailed_df = pd.concat([old_detailed_df, new_detailed_df], ignore_index=True)\n                    \n                    logging.info(f\"Saving combined data to local CSVs for {league_name}...\")\n                    combined_main_df.to_csv(MAIN_CSV_PATH, index=False)\n                    combined_detailed_df.to_csv(DETAILED_CSV_PATH, index=False)\n                else: # Still save the main lines even if detailed scraping fails\n                    logging.info(f\"Saving main lines data to local CSV for {league_name}...\")\n                    combined_main_df.to_csv(MAIN_CSV_PATH, index=False)\n            \n            if leagues_updated:\n                logging.info(\"\\n--- Finalizing and Uploading to Kaggle ---\")\n                metadata_path = os.path.join(WORKING_DIR, 'dataset-metadata.json')\n                metadata = {\"title\": \"Pinnacle Basketball Odds History\", \"id\": DATASET_SLUG, \"licenses\": [{\"name\": \"CC0-1.0\"}]}\n                with open(metadata_path, 'w') as f: json.dump(metadata, f)\n                \n                version_note = f\"Automated odds update. Leagues updated: {', '.join(leagues_updated)}.\"\n                logging.info(f\"Pushing new dataset version. {version_note}\")\n                # This safely uploads the entire working directory. Because we synced\n                # first, any untouched files are re-uploaded along with the updated ones.\n                api.dataset_create_version(folder=WORKING_DIR, version_notes=version_note, quiet=False, dir_mode='zip')\n            else:\n                logging.warning(\"No games were found for any leagues. No new version will be pushed.\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during the main pipeline: {e}\", exc_info=True)\n    finally:\n        if driver: driver.quit(); logging.info(\"Selenium driver closed.\")\n\nprint(\"\\n--- Data Pipeline Execution Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:57:00.716461Z","iopub.execute_input":"2025-08-25T05:57:00.717315Z"}},"outputs":[],"execution_count":null}]}