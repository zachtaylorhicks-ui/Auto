{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# STEP 1: KAGGLE AUTH & PYTHON DEPENDENCIES\n# ==============================================================================\nprint(\"--- Installing Python Dependencies ---\")\n!pip install -q selenium pandas kaggle\n\nimport os\nimport pandas as pd\nimport logging\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nfrom importlib import reload\n\n# Force logging to be active so we see all messages\nreload(logging)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nprint(\"\\n--- Setting up Kaggle API Authentication ---\")\napi = None\ntry:\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"KAGGLE_JSON\")\n    kaggle_dir = os.path.expanduser('~/.kaggle')\n    os.makedirs(kaggle_dir, exist_ok=True)\n    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n    with open(kaggle_json_path, 'w') as f: f.write(secret_value)\n    os.chmod(kaggle_json_path, 600)\n    \n    from kaggle.api.kaggle_api_extended import KaggleApi\n    api = KaggleApi()\n    api.authenticate()\n    print(\"Kagle API Authentication Successful.\")\nexcept Exception as e:\n    logging.critical(f\"FATAL: A critical error occurred during Kaggle setup. Error: {e}\")\n    raise\n\n# ==============================================================================\n# STEP 2: SYSTEM INSTALLATIONS (CHROME)\n# ==============================================================================\nprint(\"\\n--- Installing Google Chrome & ChromeDriver ---\")\n!sudo apt-get update > /dev/null\n!sudo apt-get install -y wget gnupg > /dev/null\n!wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -\n!sudo sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list'\n!sudo apt-get update > /dev/null\n!sudo apt-get install -y google-chrome-stable > /dev/null\n!apt-get install -y chromium-chromedriver > /dev/null\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin &>/dev/null\nprint(\"--- Chrome & ChromeDriver Setup Complete ---\")\n\n\n# ==============================================================================\n# STEP 3: YOUR PROVEN SCRAPER - UNCHANGED AND UNMODIFIED\n# ==============================================================================\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException\n\n# YOUR WORKING SCRAPER FUNCTIONS - VERBATIM\ndef get_main_wnba_lines(driver):\n    url = \"https://www.pinnacle.com/en/basketball/matchups/\"\n    logging.info(f\"Navigating to matchups page: {url}\")\n    driver.get(url)\n    try:\n        cookie_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n        logging.info(\"Found and clicked the Accept button for cookies.\"); cookie_button.click(); time.sleep(2)\n    except TimeoutException:\n        logging.warning(\"Cookie banner not found or already handled.\")\n    all_games_summary = []\n    try:\n        logging.info(\"Searching for WNBA header row...\")\n        wnba_header_row_xpath = \"//a[contains(@href, '/wnba/matchups/')]/ancestor::div[contains(@class, 'row-')]\"\n        wnba_header_row = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, wnba_header_row_xpath)))\n        all_following_rows = wnba_header_row.find_elements(By.XPATH, \"./following-sibling::div\")\n        logging.info(f\"Scanning {len(all_following_rows)} rows after WNBA header...\")\n        for row in all_following_rows:\n            row_class = row.get_attribute('class')\n            if 'row-k9ktBvvTsJ' in row_class:\n                try:\n                    game = {}\n                    link_tag = row.find_element(By.CSS_SELECTOR, \"a[href*='/basketball/wnba/']\")\n                    teams = link_tag.find_elements(By.CSS_SELECTOR, \"span.ellipsis.gameInfoLabel-EDDYv5xEfd\")\n                    game['team1'], game['team2'] = teams[0].text, teams[1].text\n                    game['game_link'] = link_tag.get_attribute('href')\n                    odds_groups = row.find_elements(By.CSS_SELECTOR, \"div.buttons-j19Jlcwsi9\")\n                    def get_text(elements, index): return elements[index].text if index < len(elements) else 'N/A'\n                    h_spans = odds_groups[0].find_elements(By.CSS_SELECTOR, \"button span\")\n                    ml_spans = odds_groups[1].find_elements(By.CSS_SELECTOR, \"span.price-r5BU0ynJha\")\n                    t_spans = odds_groups[2].find_elements(By.CSS_SELECTOR, \"button span\")\n                    game.update({'team1_moneyline': get_text(ml_spans, 0), 'team2_moneyline': get_text(ml_spans, 1),'team1_spread': get_text(h_spans, 0), 'team1_spread_odds': get_text(h_spans, 1),'team2_spread': get_text(h_spans, 2), 'team2_spread_odds': get_text(h_spans, 3),'over_total': get_text(t_spans, 0), 'over_total_odds': get_text(t_spans, 1),'under_total': get_text(t_spans, 2), 'under_total_odds': get_text(t_spans, 3)})\n                    all_games_summary.append(game)\n                except (NoSuchElementException, IndexError): continue\n            elif 'row-CTcjEjV6yK' in row_class:\n                logging.info(\"Reached the next league's header. Stopping WNBA scan.\"); break\n        return all_games_summary\n    except TimeoutException:\n        logging.error(\"Could not find the WNBA section on the main matchups page.\"); return []\n\ndef scrape_detailed_game_odds(driver, game_url):\n    logging.info(f\"Scraping detailed odds from: {game_url}\"); driver.get(game_url)\n    all_markets_data = []\n    try:\n        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.marketGroups-HjCkfKkLNt\"))); time.sleep(2)\n        market_groups = driver.find_elements(By.CSS_SELECTOR, \"div.marketGroup-wMlWprW2iC\")\n        for group in market_groups:\n            market_title = group.find_element(By.CSS_SELECTOR, \"span.titleText-BgvECQYfHf\").text\n            if not group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id]\"):\n                for btn in group.find_elements(By.CSS_SELECTOR, \"button\"):\n                    parts = btn.text.split('\\n')\n                    if len(parts) == 2: all_markets_data.append({'Market': market_title, 'Selection': parts[0], 'Odds': parts[1]})\n                continue\n            headers = [h.text for h in group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id] > li\")]\n            button_rows = group.find_elements(By.CSS_SELECTOR, \".buttonRow-zWMLOGu5YB\")\n            for row in button_rows:\n                buttons = row.find_elements(By.TAG_NAME, 'button')\n                if len(buttons) == len(headers):\n                    for i, btn in enumerate(buttons):\n                        parts = btn.text.split('\\n')\n                        if len(parts) == 2:\n                            selection_name = f\"{headers[i]} {parts[0]}\"\n                            all_markets_data.append({'Market': market_title, 'Selection': selection_name, 'Odds': parts[1]})\n    except TimeoutException:\n        logging.error(f\"Could not load market data for URL: {game_url}\")\n    return pd.DataFrame(all_markets_data)\n\n# ==============================================================================\n# STEP 4: MAIN DATA PIPELINE EXECUTION\n# ==============================================================================\nprint(\"\\n--- Starting Data Pipeline Execution ---\")\nif __name__ == \"__main__\" and api:\n    DATASET_SLUG = \"zachht/wnba-odds-history\"\n    WORKING_DIR = \"/kaggle/working\"\n    MAIN_CSV_PATH = os.path.join(WORKING_DIR, \"wnba_main_lines_history.csv\")\n    DETAILED_CSV_PATH = os.path.join(WORKING_DIR, \"wnba_detailed_odds_history.csv\")\n\n    try:\n        logging.info(f\"Downloading existing dataset: {DATASET_SLUG}...\")\n        api.dataset_download_files(DATASET_SLUG, path=WORKING_DIR, unzip=True)\n        old_main_df = pd.read_csv(MAIN_CSV_PATH)\n        old_detailed_df = pd.read_csv(DETAILED_CSV_PATH)\n        logging.info(\"Successfully loaded existing data.\")\n    except Exception as e:\n        if \"404\" in str(e): logging.warning(f\"Dataset '{DATASET_SLUG}' not found (404). Creating new files.\")\n        else: logging.warning(f\"Could not read local files (Error: {e}). Starting with fresh history.\")\n        old_main_df, old_detailed_df = pd.DataFrame(), pd.DataFrame()\n    \n    driver = None\n    try:\n        # YOUR WORKING OPTIONS ARE NOW USED HERE, VERBATIM\n        options = webdriver.ChromeOptions()\n        options.add_argument(\"--headless\")\n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--window-size=1920,1080\")\n        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\")\n        \n        driver = webdriver.Chrome(options=options)\n        new_main_lines_data = get_main_wnba_lines(driver)\n        \n        if new_main_lines_data:\n            logging.info(f\"SUCCESS: Scraper found {len(new_main_lines_data)} game(s).\")\n            scrape_timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n            new_main_df = pd.DataFrame(new_main_lines_data)\n            new_main_df['timestamp'] = scrape_timestamp\n            combined_main_df = pd.concat([old_main_df, new_main_df], ignore_index=True)\n            all_detailed_dfs = []\n            for game in new_main_lines_data:\n                detailed_df = scrape_detailed_game_odds(driver, game['game_link'])\n                if not detailed_df.empty:\n                    detailed_df['matchup'] = f\"{game['team1']} vs {game['team2']}\"\n                    all_detailed_dfs.append(detailed_df)\n            if all_detailed_dfs:\n                new_detailed_df = pd.concat(all_detailed_dfs, ignore_index=True)\n                new_detailed_df['timestamp'] = scrape_timestamp\n                combined_detailed_df = pd.concat([old_detailed_df, new_detailed_df], ignore_index=True)\n                logging.info(f\"Saving combined data to {WORKING_DIR}...\")\n                combined_main_df.to_csv(MAIN_CSV_PATH, index=False)\n                combined_detailed_df.to_csv(DETAILED_CSV_PATH, index=False)\n\n                # --- START: ADD THESE LINES TO FIX THE ERROR ---\n                import json\n                metadata_path = os.path.join(WORKING_DIR, 'dataset-metadata.json')\n                metadata = {\n                  \"title\": \"WNBA Odds History\",  # Should match your dataset's title\n                  \"id\": DATASET_SLUG,\n                  \"licenses\": [{\"name\": \"CC0-1.0\"}]\n                }\n                with open(metadata_path, 'w') as f:\n                    json.dump(metadata, f)\n                # --- END: ADDED LINES ---\n                \n                version_note = f\"Automated odds update from {scrape_timestamp} UTC, adding {len(new_main_lines_data)} games.\"\n                logging.info(\"Pushing new dataset version to Kaggle...\")\n                # This line will now succeed\n                api.dataset_create_version(folder=WORKING_DIR, version_notes=version_note, quiet=False, dir_mode='zip')\n        else:\n            logging.warning(\"Scraping finished, but no new WNBA games were found on the site.\")\n    except Exception as e:\n        logging.error(f\"An error occurred during the main scraping/uploading process: {e}\", exc_info=True)\n    finally:\n        if driver: driver.quit(); logging.info(\"Selenium driver closed.\")\nelse:\n    logging.error(\"Kaggle API object was not created. Halting execution.\")\n\nprint(\"\\n--- Data Pipeline Execution Finished ---\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# STEP 1: KAGGLE AUTH & PYTHON DEPENDENCIES\n# ==============================================================================\nprint(\"--- Installing Python Dependencies ---\")\n!pip install -q selenium pandas kaggle\n\nimport os\nimport pandas as pd\nimport logging\nimport json\nimport re\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nfrom importlib import reload\n\n# Force logging to be active so we see all messages\nreload(logging)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nprint(\"\\n--- Setting up Kaggle API Authentication ---\")\napi = None\ntry:\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"KAGGLE_JSON\")\n    kaggle_dir = os.path.expanduser('~/.kaggle')\n    os.makedirs(kaggle_dir, exist_ok=True)\n    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n    with open(kaggle_json_path, 'w') as f: f.write(secret_value)\n    os.chmod(kaggle_json_path, 600)\n    \n    from kaggle.api.kaggle_api_extended import KaggleApi\n    api = KaggleApi()\n    api.authenticate()\n    print(\"Kaggle API Authentication Successful.\")\nexcept Exception as e:\n    logging.critical(f\"FATAL: A critical error occurred during Kaggle setup. Error: {e}\")\n    raise\n\n# ==============================================================================\n# STEP 2: SYSTEM INSTALLATIONS (CHROME)\n# ==============================================================================\nprint(\"\\n--- Installing Google Chrome & ChromeDriver ---\")\n!sudo apt-get update > /dev/null\n!sudo apt-get install -y wget gnupg > /dev/null\n!wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -\n!sudo sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list'\n!sudo apt-get update > /dev/null\n!sudo apt-get install -y google-chrome-stable > /dev/null\n!apt-get install -y chromium-chromedriver > /dev/null\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin &>/dev/null\nprint(\"--- Chrome & ChromeDriver Setup Complete ---\")\n\n\n# ==============================================================================\n# STEP 3: SCRAPER FUNCTIONS USING YOUR PROVEN SELECTORS\n# ==============================================================================\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException\n\ndef get_all_leagues_and_games(driver):\n    \"\"\"\n    Scrapes the main basketball page, discovering leagues and games using your proven selectors.\n    \"\"\"\n    url = \"https://www.pinnacle.com/en/basketball/matchups/\"\n    logging.info(f\"Navigating to matchups page: {url}\")\n    driver.get(url)\n    try:\n        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))).click()\n        logging.info(\"Clicked the Accept button for cookies.\"); time.sleep(2)\n    except TimeoutException:\n        logging.warning(\"Cookie banner not found or already handled.\")\n\n    leagues_data = {}\n    current_league_name = None\n\n    try:\n        # Wait until the content is loaded, identified by the first league header\n        WebDriverWait(driver, 20).until(\n            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.row-CTcjEjV6yK\"))\n        )\n        \n        # Get all rows that are either league headers or game rows\n        all_rows = driver.find_elements(By.CSS_SELECTOR, \".contentBlock.square > div[class*='row-']\")\n        logging.info(f\"Found {len(all_rows)} total rows to process on the matchups page.\")\n\n        for row in all_rows:\n            row_class = row.get_attribute('class')\n            \n            # YOUR SELECTOR FOR A LEAGUE HEADER\n            if 'row-CTcjEjV6yK' in row_class:\n                try:\n                    league_name = row.find_element(By.CSS_SELECTOR, \"a span\").text.strip()\n                    if league_name:\n                        current_league_name = league_name\n                        leagues_data[current_league_name] = []\n                        logging.info(f\"Discovered new league section: {current_league_name}\")\n                except NoSuchElementException:\n                    continue \n\n            # YOUR SELECTOR FOR A GAME ROW\n            elif 'row-k9ktBvvTsJ' in row_class and current_league_name:\n                # THIS IS YOUR EXACT, UNMODIFIED SCRAPING LOGIC FOR A GAME ROW\n                try:\n                    game = {}\n                    link_tag = row.find_element(By.CSS_SELECTOR, \"a[href*='/basketball/']\")\n                    teams = link_tag.find_elements(By.CSS_SELECTOR, \"span.ellipsis.gameInfoLabel-EDDYv5xEfd\")\n                    game['team1'], game['team2'] = teams[0].text, teams[1].text\n                    game['game_link'] = link_tag.get_attribute('href')\n                    \n                    odds_groups = row.find_elements(By.CSS_SELECTOR, \"div.buttons-j19Jlcwsi9\")\n                    def get_text(elements, index): return elements[index].text if index < len(elements) else 'N/A'\n                    \n                    h_spans = odds_groups[0].find_elements(By.CSS_SELECTOR, \"button span\")\n                    ml_spans = odds_groups[1].find_elements(By.CSS_SELECTOR, \"span.price-r5BU0ynJha\")\n                    t_spans = odds_groups[2].find_elements(By.CSS_SELECTOR, \"button span\")\n                    \n                    game.update({'team1_moneyline': get_text(ml_spans, 0), 'team2_moneyline': get_text(ml_spans, 1),'team1_spread': get_text(h_spans, 0), 'team1_spread_odds': get_text(h_spans, 1),'team2_spread': get_text(h_spans, 2), 'team2_spread_odds': get_text(h_spans, 3),'over_total': get_text(t_spans, 0), 'over_total_odds': get_text(t_spans, 1),'under_total': get_text(t_spans, 2), 'under_total_odds': get_text(t_spans, 3)})\n                    \n                    leagues_data[current_league_name].append(game)\n                except (NoSuchElementException, IndexError):\n                    continue\n    except TimeoutException:\n        logging.error(\"Could not find any content rows on the matchups page.\")\n    \n    return leagues_data\n\n# YOUR DETAILED SCRAPER IS PERFECT AND UNCHANGED\ndef scrape_detailed_game_odds(driver, game_url):\n    logging.info(f\"Scraping detailed odds from: {game_url}\"); driver.get(game_url)\n    all_markets_data = []\n    try:\n        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.marketGroups-HjCkfKkLNt\"))); time.sleep(2)\n        market_groups = driver.find_elements(By.CSS_SELECTOR, \"div.marketGroup-wMlWprW2iC\")\n        for group in market_groups:\n            market_title = group.find_element(By.CSS_SELECTOR, \"span.titleText-BgvECQYfHf\").text\n            if not group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id]\"):\n                for btn in group.find_elements(By.CSS_SELECTOR, \"button\"):\n                    parts = btn.text.split('\\n')\n                    if len(parts) == 2: all_markets_data.append({'Market': market_title, 'Selection': parts[0], 'Odds': parts[1]})\n                continue\n            headers = [h.text for h in group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id] > li\")]\n            button_rows = group.find_elements(By.CSS_SELECTOR, \".buttonRow-zWMLOGu5YB\")\n            for row in button_rows:\n                buttons = row.find_elements(By.TAG_NAME, 'button')\n                if len(buttons) == len(headers):\n                    for i, btn in enumerate(buttons):\n                        parts = btn.text.split('\\n')\n                        if len(parts) == 2:\n                            selection_name = f\"{headers[i]} {parts[0]}\"\n                            all_markets_data.append({'Market': market_title, 'Selection': selection_name, 'Odds': parts[1]})\n    except TimeoutException:\n        logging.error(f\"Could not load market data for URL: {game_url}\")\n    return pd.DataFrame(all_markets_data)\n\ndef to_slug(name):\n    \"\"\"Converts a league name like 'WNBA Summer League' to 'wnba_summer_league' for filenames.\"\"\"\n    return re.sub(r'[^a-z0-9]+', '_', name.lower()).strip('_')\n\n# ==============================================================================\n# STEP 4: MAIN DATA PIPELINE EXECUTION\n# ==============================================================================\nprint(\"\\n--- Starting Data Pipeline Execution ---\")\nif __name__ == \"__main__\" and api:\n    DATASET_SLUG = \"zachht/wnba-odds-history\" \n    WORKING_DIR = \"/kaggle/working\"\n    \n    driver = None\n    leagues_updated = []\n    try:\n        options = webdriver.ChromeOptions()\n        options.add_argument(\"--headless\"); options.add_argument(\"--no-sandbox\"); options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--window-size=1920,1080\")\n        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\")\n        driver = webdriver.Chrome(options=options)\n        \n        all_leagues_games = get_all_leagues_and_games(driver)\n\n        if not all_leagues_games:\n            logging.warning(\"Scraping finished, but no leagues were found on the site.\")\n        else:\n            for league_name, new_main_lines_data in all_leagues_games.items():\n                if not new_main_lines_data:\n                    logging.info(f\"No games found for league: {league_name}. Skipping.\")\n                    continue\n\n                logging.info(f\"\\n--- Processing League: {league_name} ({len(new_main_lines_data)} games found) ---\")\n                leagues_updated.append(league_name)\n                league_slug = to_slug(league_name)\n\n                MAIN_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_main_lines.csv\")\n                DETAILED_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_detailed_odds.csv\")\n\n                try:\n                    logging.info(f\"Downloading existing files for {league_name}...\")\n                    api.dataset_download_file(DATASET_SLUG, file_name=os.path.basename(MAIN_CSV_PATH), path=WORKING_DIR)\n                    api.dataset_download_file(DATASET_SLUG, file_name=os.path.basename(DETAILED_CSV_PATH), path=WORKING_DIR)\n                    old_main_df = pd.read_csv(MAIN_CSV_PATH)\n                    old_detailed_df = pd.read_csv(DETAILED_CSV_PATH)\n                    logging.info(\"Successfully loaded existing data.\")\n                except Exception:\n                    logging.warning(f\"Could not load existing data for {league_name}. Starting with fresh history files.\")\n                    old_main_df, old_detailed_df = pd.DataFrame(), pd.DataFrame()\n\n                scrape_timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n                new_main_df = pd.DataFrame(new_main_lines_data)\n                new_main_df['timestamp'] = scrape_timestamp\n                combined_main_df = pd.concat([old_main_df, new_main_df], ignore_index=True)\n                \n                all_detailed_dfs = []\n                for game in new_main_lines_data:\n                    detailed_df = scrape_detailed_game_odds(driver, game['game_link'])\n                    if not detailed_df.empty:\n                        detailed_df['matchup'] = f\"{game['team1']} vs {game['team2']}\"\n                        all_detailed_dfs.append(detailed_df)\n                \n                if all_detailed_dfs:\n                    new_detailed_df = pd.concat(all_detailed_dfs, ignore_index=True)\n                    new_detailed_df['timestamp'] = scrape_timestamp\n                    combined_detailed_df = pd.concat([old_detailed_df, new_detailed_df], ignore_index=True)\n                    \n                    logging.info(f\"Saving combined data to local CSVs for {league_name}...\")\n                    combined_main_df.to_csv(MAIN_CSV_PATH, index=False)\n                    combined_detailed_df.to_csv(DETAILED_CSV_PATH, index=False)\n            \n            if leagues_updated:\n                logging.info(\"\\n--- Finalizing and Uploading to Kaggle ---\")\n                metadata_path = os.path.join(WORKING_DIR, 'dataset-metadata.json')\n                metadata = {\"title\": \"Pinnacle Basketball Odds History\", \"id\": DATASET_SLUG, \"licenses\": [{\"name\": \"CC0-1.0\"}]}\n                with open(metadata_path, 'w') as f: json.dump(metadata, f)\n                \n                version_note = f\"Automated odds update. Leagues updated: {', '.join(leagues_updated)}.\"\n                logging.info(f\"Pushing new dataset version. {version_note}\")\n                api.dataset_create_version(folder=WORKING_DIR, version_notes=version_note, quiet=False, dir_mode='zip')\n            else:\n                logging.warning(\"No games were found for any leagues. No new version will be pushed.\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during the main pipeline: {e}\", exc_info=True)\n    finally:\n        if driver: driver.quit(); logging.info(\"Selenium driver closed.\")\n\nprint(\"\\n--- Data Pipeline Execution Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T20:38:41.919946Z","iopub.execute_input":"2025-08-24T20:38:41.920725Z","iopub.status.idle":"2025-08-24T20:42:30.838773Z","shell.execute_reply.started":"2025-08-24T20:38:41.920689Z","shell.execute_reply":"2025-08-24T20:42:30.837619Z"}},"outputs":[{"name":"stdout","text":"--- Installing Python Dependencies ---\n\n--- Setting up Kaggle API Authentication ---\nKaggle API Authentication Successful.\n\n--- Installing Google Chrome & ChromeDriver ---\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nWarning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\nOK\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n--- Chrome & ChromeDriver Setup Complete ---\n\n--- Starting Data Pipeline Execution ---\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:39:03,596 - INFO - Navigating to matchups page: https://www.pinnacle.com/en/basketball/matchups/\n2025-08-24 20:39:14,545 - WARNING - Cookie banner not found or already handled.\n2025-08-24 20:39:14,578 - INFO - Found 30 total rows to process on the matchups page.\n2025-08-24 20:39:14,621 - INFO - Discovered new league section: WNBA\n2025-08-24 20:39:14,892 - INFO - Discovered new league section: BRAZIL - CAMPEONATO PAULISTA\n2025-08-24 20:39:15,625 - INFO - Discovered new league section: BRAZIL - CAMP CARIOCA U19\n2025-08-24 20:39:15,873 - INFO - Discovered new league section: FIBA - AMERICUP\n2025-08-24 20:39:16,143 - INFO - Discovered new league section: MEXICO - LIGA NACIONAL DE BALONCESTO PROFESIONAL\n2025-08-24 20:39:16,615 - INFO - Discovered new league section: BRAZIL - PAULISTA FPB U20\n2025-08-24 20:39:16,900 - INFO - Discovered new league section: FIBA - AMERICUP\n2025-08-24 20:39:17,594 - INFO - Discovered new league section: WORLD - CLUB FRIENDLIES\n2025-08-24 20:39:17,846 - INFO - Discovered new league section: FIBA - AMERICUP\n2025-08-24 20:39:18,096 - INFO - Discovered new league section: FIBA - EUROBASKET\n2025-08-24 20:39:19,379 - INFO - \n--- Processing League: WNBA (1 games found) ---\n2025-08-24 20:39:19,380 - INFO - Downloading existing files for WNBA...\n2025-08-24 20:39:19,492 - WARNING - Could not load existing data for WNBA. Starting with fresh history files.\n2025-08-24 20:39:19,504 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/wnba/indiana-fever-vs-minnesota-lynx/1613797001/\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:39:25,445 - INFO - Saving combined data to local CSVs for WNBA...\n2025-08-24 20:39:25,453 - INFO - \n--- Processing League: BRAZIL - CAMPEONATO PAULISTA (3 games found) ---\n2025-08-24 20:39:25,453 - INFO - Downloading existing files for BRAZIL - CAMPEONATO PAULISTA...\n2025-08-24 20:39:25,537 - WARNING - Could not load existing data for BRAZIL - CAMPEONATO PAULISTA. Starting with fresh history files.\n2025-08-24 20:39:25,540 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/brazil-campeonato-paulista/sao-jose-vs-paulistano/1613776437/\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:39:45,923 - ERROR - Could not load market data for URL: https://www.pinnacle.com/en/basketball/brazil-campeonato-paulista/sao-jose-vs-paulistano/1613776437/\n2025-08-24 20:39:45,925 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/brazil-campeonato-paulista/franca-basquetbol-clube-vs-corinthians-paulista/1613776438/\n2025-08-24 20:40:06,211 - ERROR - Could not load market data for URL: https://www.pinnacle.com/en/basketball/brazil-campeonato-paulista/franca-basquetbol-clube-vs-corinthians-paulista/1613776438/\n2025-08-24 20:40:06,213 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/brazil-campeonato-paulista/bauru-basket-sp-vs-liga-sorocabana/1613973422/\n2025-08-24 20:40:26,575 - ERROR - Could not load market data for URL: https://www.pinnacle.com/en/basketball/brazil-campeonato-paulista/bauru-basket-sp-vs-liga-sorocabana/1613973422/\n2025-08-24 20:40:26,576 - INFO - \n--- Processing League: BRAZIL - CAMP CARIOCA U19 (1 games found) ---\n2025-08-24 20:40:26,577 - INFO - Downloading existing files for BRAZIL - CAMP CARIOCA U19...\n2025-08-24 20:40:26,685 - WARNING - Could not load existing data for BRAZIL - CAMP CARIOCA U19. Starting with fresh history files.\n2025-08-24 20:40:26,689 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/brazil-camp-carioca-u19/escolinha-de-esportes-passo-zero-vs-jacarepagua/1613967630/\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:40:47,174 - ERROR - Could not load market data for URL: https://www.pinnacle.com/en/basketball/brazil-camp-carioca-u19/escolinha-de-esportes-passo-zero-vs-jacarepagua/1613967630/\n2025-08-24 20:40:47,176 - INFO - \n--- Processing League: FIBA - AMERICUP (1 games found) ---\n2025-08-24 20:40:47,176 - INFO - Downloading existing files for FIBA - AMERICUP...\n2025-08-24 20:40:47,276 - WARNING - Could not load existing data for FIBA - AMERICUP. Starting with fresh history files.\n2025-08-24 20:40:47,279 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-americup/canada-vs-puerto-rico/1613591100/\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:41:07,813 - ERROR - Could not load market data for URL: https://www.pinnacle.com/en/basketball/fiba-americup/canada-vs-puerto-rico/1613591100/\n2025-08-24 20:41:07,814 - INFO - \n--- Processing League: MEXICO - LIGA NACIONAL DE BALONCESTO PROFESIONAL (2 games found) ---\n2025-08-24 20:41:07,815 - INFO - Downloading existing files for MEXICO - LIGA NACIONAL DE BALONCESTO PROFESIONAL...\n2025-08-24 20:41:07,892 - WARNING - Could not load existing data for MEXICO - LIGA NACIONAL DE BALONCESTO PROFESIONAL. Starting with fresh history files.\n2025-08-24 20:41:07,896 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/mexico-liga-nacional-de-baloncesto-profesional/fuerza-regia-de-monterrey-vs-mineros-zacatecas/1613796679/\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:41:13,162 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/mexico-liga-nacional-de-baloncesto-profesional/diablos-rojos-vs-abejas-de-leon/1613797005/\n2025-08-24 20:41:18,273 - INFO - Saving combined data to local CSVs for MEXICO - LIGA NACIONAL DE BALONCESTO PROFESIONAL...\n2025-08-24 20:41:18,276 - INFO - \n--- Processing League: BRAZIL - PAULISTA FPB U20 (1 games found) ---\n2025-08-24 20:41:18,277 - INFO - Downloading existing files for BRAZIL - PAULISTA FPB U20...\n2025-08-24 20:41:18,375 - WARNING - Could not load existing data for BRAZIL - PAULISTA FPB U20. Starting with fresh history files.\n2025-08-24 20:41:18,378 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/brazil-paulista-fpb-u20/sao-jose-basketball-vs-paulistano/1613870162/\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:41:38,815 - ERROR - Could not load market data for URL: https://www.pinnacle.com/en/basketball/brazil-paulista-fpb-u20/sao-jose-basketball-vs-paulistano/1613870162/\n2025-08-24 20:41:38,817 - INFO - \n--- Processing League: WORLD - CLUB FRIENDLIES (1 games found) ---\n2025-08-24 20:41:38,818 - INFO - Downloading existing files for WORLD - CLUB FRIENDLIES...\n2025-08-24 20:41:38,907 - WARNING - Could not load existing data for WORLD - CLUB FRIENDLIES. Starting with fresh history files.\n2025-08-24 20:41:38,911 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/world-club-friendlies/changwon-lg-sakers-vs-up-fighting-maroons/1614058987/\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:41:43,904 - INFO - Saving combined data to local CSVs for WORLD - CLUB FRIENDLIES...\n2025-08-24 20:41:43,907 - INFO - \n--- Processing League: FIBA - EUROBASKET (6 games found) ---\n2025-08-24 20:41:43,908 - INFO - Downloading existing files for FIBA - EUROBASKET...\n2025-08-24 20:41:43,994 - WARNING - Could not load existing data for FIBA - EUROBASKET. Starting with fresh history files.\n2025-08-24 20:41:43,997 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/great-britain-vs-lithuania/1613559800/\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:41:48,777 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/czech-republic-vs-portugal/1613551688/\n2025-08-24 20:41:53,370 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/montenegro-vs-germany/1613559801/\n2025-08-24 20:41:57,867 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/latvia-vs-turkiye/1613551689/\n2025-08-24 20:42:16,636 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/sweden-vs-finland/1613551690/\n2025-08-24 20:42:21,544 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/serbia-vs-estonia/1613561299/\n2025-08-24 20:42:26,164 - INFO - Saving combined data to local CSVs for FIBA - EUROBASKET...\n2025-08-24 20:42:26,167 - INFO - \n--- Finalizing and Uploading to Kaggle ---\n2025-08-24 20:42:26,168 - INFO - Pushing new dataset version. Automated odds update. Leagues updated: WNBA, BRAZIL - CAMPEONATO PAULISTA, BRAZIL - CAMP CARIOCA U19, FIBA - AMERICUP, MEXICO - LIGA NACIONAL DE BALONCESTO PROFESIONAL, BRAZIL - PAULISTA FPB U20, WORLD - CLUB FRIENDLIES, FIBA - EUROBASKET.\n","output_type":"stream"},{"name":"stdout","text":"Starting upload for file mexico_liga_nacional_de_baloncesto_profesional_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8.96k/8.96k [00:00<00:00, 37.8kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: mexico_liga_nacional_de_baloncesto_profesional_detailed_odds.csv (9KB)\nStarting upload for file .virtual_documents.zip\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 22.0/22.0 [00:00<00:00, 117B/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: .virtual_documents.zip (22B)\nStarting upload for file fiba_eurobasket_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1.26k/1.26k [00:00<00:00, 7.04kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: fiba_eurobasket_main_lines.csv (1KB)\nStarting upload for file world_club_friendlies_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.63k/4.63k [00:00<00:00, 26.6kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: world_club_friendlies_detailed_odds.csv (5KB)\nStarting upload for file world_club_friendlies_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 414/414 [00:00<00:00, 2.30kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: world_club_friendlies_main_lines.csv (414B)\nStarting upload for file fiba_eurobasket_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2.75k/2.75k [00:00<00:00, 14.5kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: fiba_eurobasket_detailed_odds.csv (3KB)\nStarting upload for file wnba_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 995/995 [00:00<00:00, 5.56kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: wnba_detailed_odds.csv (995B)\nStarting upload for file wnba_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 379/379 [00:00<00:00, 2.10kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: wnba_main_lines.csv (379B)\nStarting upload for file mexico_liga_nacional_de_baloncesto_profesional_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 684/684 [00:00<00:00, 3.67kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: mexico_liga_nacional_de_baloncesto_profesional_main_lines.csv (684B)\n","output_type":"stream"},{"name":"stderr","text":"2025-08-24 20:42:30,832 - INFO - Selenium driver closed.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Data Pipeline Execution Finished ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:10:44.970044Z","iopub.execute_input":"2025-08-25T05:10:44.970244Z","iopub.status.idle":"2025-08-25T05:10:44.982762Z","shell.execute_reply.started":"2025-08-25T05:10:44.970224Z","shell.execute_reply":"2025-08-25T05:10:44.981410Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36/2994937114.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    kaggle kernels init -p /path/to/your/notebook/folder\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2994937114.py, line 1)","output_type":"error"}],"execution_count":1}]}