{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# STEP 1: KAGGLE AUTH & PYTHON DEPENDENCIES\n# ==============================================================================\nprint(\"--- Installing Python Dependencies ---\")\n!pip install -q selenium pandas kaggle # <--- REVERT BACK TO THIS\n# ... rest of your imports\n\nimport os\nimport pandas as pd\nimport logging\nimport json\nimport re\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nfrom importlib import reload\n\n# Force logging to be active so we see all messages\nreload(logging)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nprint(\"\\n--- Setting up Kaggle API Authentication ---\")\napi = None\ntry:\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"KAGGLE_JSON\")\n    kaggle_dir = os.path.expanduser('~/.kaggle')\n    os.makedirs(kaggle_dir, exist_ok=True)\n    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n    with open(kaggle_json_path, 'w') as f: f.write(secret_value)\n    os.chmod(kaggle_json_path, 600)\n    \n    from kaggle.api.kaggle_api_extended import KaggleApi\n    api = KaggleApi()\n    api.authenticate()\n    print(\"Kaggle API Authentication Successful.\")\nexcept Exception as e:\n    logging.critical(f\"FATAL: A critical error occurred during Kaggle setup. Error: {e}\")\n    raise\n\n# ==============================================================================\n# STEP 2: SYSTEM INSTALLATIONS (CHROME)\n# ==============================================================================\nprint(\"\\n--- Installing Google Chrome & ChromeDriver ---\")\n# Using quiet flags to keep the log clean\n!sudo apt-get update > /dev/null\n!sudo apt-get install -y wget gnupg > /dev/null\n!wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -\n!sudo sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list'\n!sudo apt-get update > /dev/null\n!sudo apt-get install -y google-chrome-stable > /dev/null\n!apt-get install -y chromium-chromedriver > /dev/null\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin &>/dev/null\nprint(\"--- Chrome & ChromeDriver Setup Complete ---\")\n\n\n# ==============================================================================\n# STEP 3: SCRAPER FUNCTIONS (WITH ROBUSTNESS FIXES)\n# ==============================================================================\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException\n\ndef get_all_leagues_and_games(driver):\n    \"\"\"\n    Scrapes the main basketball page with robust waits and debugging.\n    \"\"\"\n    url = \"https://www.pinnacle.com/en/basketball/matchups/\"\n    logging.info(f\"Navigating to matchups page: {url}\")\n    driver.get(url)\n\n    # Handle cookie banner if it appears\n    try:\n        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))).click()\n        logging.info(\"Clicked the Accept button for cookies.\"); time.sleep(2)\n    except TimeoutException:\n        logging.warning(\"Cookie banner not found or already handled.\")\n\n    leagues_data = {}\n    current_league_name = None\n\n    try:\n        # ROBUSTNESS FIX #1: Wait for the main content container to be present.\n        # This is the single most important element to wait for.\n        # Selector targets the block that holds all the leagues and games.\n        content_container_selector = (By.CSS_SELECTOR, \".contentBlock.square\")\n        logging.info(\"Waiting for the main content container to load...\")\n        WebDriverWait(driver, 30).until(\n            EC.presence_of_element_located(content_container_selector)\n        )\n        logging.info(\"Main content container found. Proceeding to scrape rows.\")\n        \n        # Give the JS a brief moment to render everything after the container is found\n        time.sleep(2)\n\n        all_rows = driver.find_elements(By.CSS_SELECTOR, \".contentBlock.square > div[class*='row-']\")\n        if not all_rows:\n            logging.error(\"Content container was found, but it contains no game or league rows.\")\n            # ROBUSTNESS FIX #2: Save page source for debugging if rows are missing\n            with open(\"debug_page_no_rows.html\", \"w\", encoding=\"utf-8\") as f:\n                f.write(driver.page_source)\n            logging.info(\"Saved debug_page_no_rows.html to output for analysis.\")\n            return {}\n\n        logging.info(f\"Found {len(all_rows)} total rows to process on the matchups page.\")\n\n        for row in all_rows:\n            row_class = row.get_attribute('class')\n            \n            if 'row-CTcjEjV6yK' in row_class:\n                try:\n                    league_name = row.find_element(By.CSS_SELECTOR, \"a span\").text.strip()\n                    if league_name:\n                        current_league_name = league_name\n                        leagues_data[current_league_name] = []\n                        logging.info(f\"Discovered new league section: {current_league_name}\")\n                except NoSuchElementException:\n                    continue \n\n            elif 'row-k9ktBvvTsJ' in row_class and current_league_name:\n                try:\n                    game = {}\n                    link_tag = row.find_element(By.CSS_SELECTOR, \"a[href*='/basketball/']\")\n                    teams = link_tag.find_elements(By.CSS_SELECTOR, \"span.ellipsis.gameInfoLabel-EDDYv5xEfd\")\n                    game['team1'], game['team2'] = teams[0].text, teams[1].text\n                    game['game_link'] = link_tag.get_attribute('href')\n                    \n                    odds_groups = row.find_elements(By.CSS_SELECTOR, \"div.buttons-j19Jlcwsi9\")\n                    def get_text(elements, index): return elements[index].text if index < len(elements) else 'N/A'\n                    \n                    h_spans = odds_groups[0].find_elements(By.CSS_SELECTOR, \"button span\")\n                    ml_spans = odds_groups[1].find_elements(By.CSS_SELECTOR, \"span.price-r5BU0ynJha\")\n                    t_spans = odds_groups[2].find_elements(By.CSS_SELECTOR, \"button span\")\n                    \n                    game.update({'team1_moneyline': get_text(ml_spans, 0), 'team2_moneyline': get_text(ml_spans, 1),'team1_spread': get_text(h_spans, 0), 'team1_spread_odds': get_text(h_spans, 1),'team2_spread': get_text(h_spans, 2), 'team2_spread_odds': get_text(h_spans, 3),'over_total': get_text(t_spans, 0), 'over_total_odds': get_text(t_spans, 1),'under_total': get_text(t_spans, 2), 'under_total_odds': get_text(t_spans, 3)})\n                    \n                    leagues_data[current_league_name].append(game)\n                except (NoSuchElementException, IndexError):\n                    continue\n\n    except TimeoutException:\n        # ROBUSTNESS FIX #2 (Primary use case): Save page source if main container never loads\n        logging.error(\"FATAL: Timed out waiting for the main content container. The page may be blocked or changed.\")\n        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n            f.write(driver.page_source)\n        logging.info(\"Saved debug_page.html to output. This file will show what the scraper saw (e.g., a CAPTCHA).\")\n    \n    return leagues_data\n\ndef scrape_detailed_game_odds(driver, game_url):\n    logging.info(f\"Scraping detailed odds from: {game_url}\")\n    driver.get(game_url)\n    all_markets_data = []\n    try:\n        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.marketGroups-HjCkfKkLNt\"))); time.sleep(2)\n        market_groups = driver.find_elements(By.CSS_SELECTOR, \"div.marketGroup-wMlWprW2iC\")\n        for group in market_groups:\n            market_title = group.find_element(By.CSS_SELECTOR, \"span.titleText-BgvECQYfHf\").text\n            if not group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id]\"):\n                for btn in group.find_elements(By.CSS_SELECTOR, \"button\"):\n                    parts = btn.text.split('\\n')\n                    if len(parts) == 2: all_markets_data.append({'Market': market_title, 'Selection': parts[0], 'Odds': parts[1]})\n                continue\n            headers = [h.text for h in group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id] > li\")]\n            button_rows = group.find_elements(By.CSS_SELECTOR, \".buttonRow-zWMLOGu5YB\")\n            for row in button_rows:\n                buttons = row.find_elements(By.TAG_NAME, 'button')\n                if len(buttons) == len(headers):\n                    for i, btn in enumerate(buttons):\n                        parts = btn.text.split('\\n')\n                        if len(parts) == 2:\n                            selection_name = f\"{headers[i]} {parts[0]}\"\n                            all_markets_data.append({'Market': market_title, 'Selection': selection_name, 'Odds': parts[1]})\n    except TimeoutException:\n        logging.error(f\"Could not load market data for URL: {game_url}\")\n    return pd.DataFrame(all_markets_data)\n\ndef to_slug(name):\n    return re.sub(r'[^a-z0-9]+', '_', name.lower()).strip('_')\n\n# ==============================================================================\n# STEP 4: MAIN DATA PIPELINE EXECUTION\n# ==============================================================================\nprint(\"\\n--- Starting Data Pipeline Execution ---\")\nif __name__ == \"__main__\" and api:\n    DATASET_SLUG = \"zachht/wnba-odds-history\" \n    WORKING_DIR = \"/kaggle/working\"\n    \n    driver = None\n    leagues_updated = []\n    try:\n        # ROBUSTNESS FIX #3: More realistic browser options\n        # --- REPLACE WITH THIS BLOCK ---\n        # --- REPLACE WITH THIS NEW, MORE STABLE BLOCK ---\n        from selenium import webdriver\n\n        logging.info(\"Initializing a smarter, stealthier Selenium driver...\")\n        options = webdriver.ChromeOptions()\n\n        # Use the new headless mode which is harder to detect\n        options.add_argument(\"--headless=new\") \n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--window-size=1920,1080\")\n        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\")\n\n        # Key options to make Selenium look less like a bot\n        options.add_argument('--disable-blink-features=AutomationControlled')\n        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n        options.add_experimental_option('useAutomationExtension', False)\n\n        driver = webdriver.Chrome(options=options)\n        # We also need to execute a command to fool the bot detector\n        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n            'source': '''\n                Object.defineProperty(navigator, 'webdriver', {\n                  get: () => undefined\n                })\n            '''\n        })\n\n        logging.info(\"Smarter Selenium driver initialized.\")\n        \n        \n        all_leagues_games = get_all_leagues_and_games(driver)\n\n        if not all_leagues_games:\n            logging.warning(\"Scraping finished, but no leagues were found on the site. Check debug files if they were created.\")\n        else:\n            for league_name, new_main_lines_data in all_leagues_games.items():\n                if not new_main_lines_data:\n                    logging.info(f\"No games found for league: {league_name}. Skipping.\")\n                    continue\n\n                logging.info(f\"\\n--- Processing League: {league_name} ({len(new_main_lines_data)} games found) ---\")\n                leagues_updated.append(league_name)\n                league_slug = to_slug(league_name)\n\n                MAIN_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_main_lines.csv\")\n                DETAILED_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_detailed_odds.csv\")\n\n                try:\n                    logging.info(f\"Downloading existing files for {league_name}...\")\n                    api.dataset_download_file(DATASET_SLUG, file_name=os.path.basename(MAIN_CSV_PATH), path=WORKING_DIR)\n                    api.dataset_download_file(DATASET_SLUG, file_name=os.path.basename(DETAILED_CSV_PATH), path=WORKING_DIR)\n                    old_main_df = pd.read_csv(MAIN_CSV_PATH)\n                    old_detailed_df = pd.read_csv(DETAILED_CSV_PATH)\n                    logging.info(\"Successfully loaded existing data.\")\n                except Exception:\n                    logging.warning(f\"Could not load existing data for {league_name}. Starting with fresh history files.\")\n                    old_main_df, old_detailed_df = pd.DataFrame(), pd.DataFrame()\n\n                scrape_timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n                new_main_df = pd.DataFrame(new_main_lines_data)\n                new_main_df['timestamp'] = scrape_timestamp\n                combined_main_df = pd.concat([old_main_df, new_main_df], ignore_index=True)\n                \n                all_detailed_dfs = []\n                for game in new_main_lines_data:\n                    detailed_df = scrape_detailed_game_odds(driver, game['game_link'])\n                    if not detailed_df.empty:\n                        detailed_df['matchup'] = f\"{game['team1']} vs {game['team2']}\"\n                        all_detailed_dfs.append(detailed_df)\n                \n                if all_detailed_dfs:\n                    new_detailed_df = pd.concat(all_detailed_dfs, ignore_index=True)\n                    new_detailed_df['timestamp'] = scrape_timestamp\n                    combined_detailed_df = pd.concat([old_detailed_df, new_detailed_df], ignore_index=True)\n                    \n                    logging.info(f\"Saving combined data to local CSVs for {league_name}...\")\n                    combined_main_df.to_csv(MAIN_CSV_PATH, index=False)\n                    combined_detailed_df.to_csv(DETAILED_CSV_PATH, index=False)\n            \n            if leagues_updated:\n                logging.info(\"\\n--- Finalizing and Uploading to Kaggle ---\")\n                metadata_path = os.path.join(WORKING_DIR, 'dataset-metadata.json')\n                metadata = {\"title\": \"Pinnacle Basketball Odds History\", \"id\": DATASET_SLUG, \"licenses\": [{\"name\": \"CC0-1.0\"}]}\n                with open(metadata_path, 'w') as f: json.dump(metadata, f)\n                \n                version_note = f\"Automated odds update. Leagues updated: {', '.join(leagues_updated)}.\"\n                logging.info(f\"Pushing new dataset version. {version_note}\")\n                api.dataset_create_version(folder=WORKING_DIR, version_notes=version_note, quiet=False, dir_mode='zip')\n            else:\n                logging.warning(\"No games were found for any leagues. No new version will be pushed.\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during the main pipeline: {e}\", exc_info=True)\n    finally:\n        if driver: driver.quit(); logging.info(\"Selenium driver closed.\")\n\nprint(\"\\n--- Data Pipeline Execution Finished ---\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:53:11.942052Z","iopub.execute_input":"2025-08-25T05:53:11.942475Z","iopub.status.idle":"2025-08-25T05:55:20.831304Z","shell.execute_reply.started":"2025-08-25T05:53:11.942446Z","shell.execute_reply":"2025-08-25T05:55:20.830273Z"}},"outputs":[{"name":"stdout","text":"--- Installing Python Dependencies ---\n\n--- Setting up Kaggle API Authentication ---\nKaggle API Authentication Successful.\n\n--- Installing Google Chrome & ChromeDriver ---\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nWarning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\nOK\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:53:34,450 - INFO - Initializing a smarter, stealthier Selenium driver...\n","output_type":"stream"},{"name":"stdout","text":"--- Chrome & ChromeDriver Setup Complete ---\n\n--- Starting Data Pipeline Execution ---\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:53:34,949 - INFO - Smarter Selenium driver initialized.\n2025-08-25 05:53:34,950 - INFO - Navigating to matchups page: https://www.pinnacle.com/en/basketball/matchups/\n2025-08-25 05:53:45,434 - WARNING - Cookie banner not found or already handled.\n2025-08-25 05:53:45,435 - INFO - Waiting for the main content container to load...\n2025-08-25 05:53:45,452 - INFO - Main content container found. Proceeding to scrape rows.\n2025-08-25 05:53:47,471 - INFO - Found 26 total rows to process on the matchups page.\n2025-08-25 05:53:47,520 - INFO - Discovered new league section: WNBA\n2025-08-25 05:53:47,897 - INFO - Discovered new league section: BRAZIL - PAULISTA FPB U20\n2025-08-25 05:53:48,181 - INFO - Discovered new league section: FIBA - AMERICUP\n2025-08-25 05:53:48,446 - INFO - Discovered new league section: WORLD - CLUB FRIENDLIES\n2025-08-25 05:53:48,717 - INFO - Discovered new league section: WNBA\n2025-08-25 05:53:48,998 - INFO - Discovered new league section: FIBA - AMERICUP\n2025-08-25 05:53:49,291 - INFO - Discovered new league section: FIBA - EUROBASKET\n2025-08-25 05:53:50,793 - INFO - Discovered new league section: FIBA - EUROBASKET\n2025-08-25 05:53:52,193 - INFO - \n--- Processing League: WNBA (1 games found) ---\n2025-08-25 05:53:52,194 - INFO - Downloading existing files for WNBA...\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\nDataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:53:52,918 - INFO - Successfully loaded existing data.\n2025-08-25 05:53:52,927 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/wnba/las-vegas-aces-vs-chicago-sky/1613894209/\n2025-08-25 05:53:58,117 - INFO - Saving combined data to local CSVs for WNBA...\n2025-08-25 05:53:58,126 - INFO - \n--- Processing League: BRAZIL - PAULISTA FPB U20 (1 games found) ---\n2025-08-25 05:53:58,127 - INFO - Downloading existing files for BRAZIL - PAULISTA FPB U20...\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\nDataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:53:59,411 - INFO - Successfully loaded existing data.\n2025-08-25 05:53:59,415 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/brazil-paulista-fpb-u20/sao-jose-basketball-vs-paulistano/1613870162/\n2025-08-25 05:54:04,298 - INFO - Saving combined data to local CSVs for BRAZIL - PAULISTA FPB U20...\n2025-08-25 05:54:04,302 - INFO - \n--- Processing League: FIBA - AMERICUP (1 games found) ---\n2025-08-25 05:54:04,303 - INFO - Downloading existing files for FIBA - AMERICUP...\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\nDataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:54:05,031 - INFO - Successfully loaded existing data.\n2025-08-25 05:54:05,035 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-americup/canada-vs-puerto-rico/1613591100/\n2025-08-25 05:54:09,250 - INFO - Saving combined data to local CSVs for FIBA - AMERICUP...\n2025-08-25 05:54:09,253 - INFO - \n--- Processing League: WORLD - CLUB FRIENDLIES (1 games found) ---\n2025-08-25 05:54:09,254 - INFO - Downloading existing files for WORLD - CLUB FRIENDLIES...\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\nDataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:54:09,974 - INFO - Successfully loaded existing data.\n2025-08-25 05:54:09,976 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/world-club-friendlies/changwon-lg-sakers-vs-up-fighting-maroons/1614058987/\n2025-08-25 05:54:13,923 - INFO - Saving combined data to local CSVs for WORLD - CLUB FRIENDLIES...\n2025-08-25 05:54:13,927 - INFO - \n--- Processing League: FIBA - EUROBASKET (6 games found) ---\n2025-08-25 05:54:13,927 - INFO - Downloading existing files for FIBA - EUROBASKET...\n","output_type":"stream"},{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\nDataset URL: https://www.kaggle.com/datasets/zachht/wnba-odds-history\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:54:14,736 - INFO - Successfully loaded existing data.\n2025-08-25 05:54:14,739 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/georgia-vs-spain/1613561311/\n2025-08-25 05:54:18,911 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/israel-vs-iceland/1613551699/\n2025-08-25 05:54:37,246 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/belgium-vs-france/1613561310/\n2025-08-25 05:54:57,928 - ERROR - Could not load market data for URL: https://www.pinnacle.com/en/basketball/fiba-eurobasket/belgium-vs-france/1613561310/\n2025-08-25 05:54:57,930 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/bosnia-herzegovina-vs-cyprus/1613559812/\n2025-08-25 05:55:02,692 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/greece-vs-italy/1613551698/\n2025-08-25 05:55:07,260 - INFO - Scraping detailed odds from: https://www.pinnacle.com/en/basketball/fiba-eurobasket/slovenia-vs-poland/1613551697/\n2025-08-25 05:55:11,800 - INFO - Saving combined data to local CSVs for FIBA - EUROBASKET...\n2025-08-25 05:55:11,804 - INFO - \n--- Finalizing and Uploading to Kaggle ---\n2025-08-25 05:55:11,806 - INFO - Pushing new dataset version. Automated odds update. Leagues updated: WNBA, BRAZIL - PAULISTA FPB U20, FIBA - AMERICUP, WORLD - CLUB FRIENDLIES, FIBA - EUROBASKET.\n","output_type":"stream"},{"name":"stdout","text":"Starting upload for file fiba_americup_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 960/960 [00:00<00:00, 2.76kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: fiba_americup_detailed_odds.csv (960B)\nStarting upload for file .virtual_documents.zip\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 22.0/22.0 [00:00<00:00, 64.7B/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: .virtual_documents.zip (22B)\nStarting upload for file brazil_paulista_fpb_u20_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 616/616 [00:00<00:00, 1.84kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: brazil_paulista_fpb_u20_main_lines.csv (616B)\nStarting upload for file fiba_eurobasket_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.41k/4.41k [00:00<00:00, 13.2kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: fiba_eurobasket_detailed_odds.csv (4KB)\nStarting upload for file world_club_friendlies_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9.21k/9.21k [00:00<00:00, 28.9kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: world_club_friendlies_detailed_odds.csv (9KB)\nStarting upload for file wnba_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1.85k/1.85k [00:00<00:00, 5.48kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: wnba_detailed_odds.csv (2KB)\nStarting upload for file wnba_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 556/556 [00:00<00:00, 1.61kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: wnba_main_lines.csv (556B)\nStarting upload for file world_club_friendlies_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 640/640 [00:00<00:00, 1.87kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: world_club_friendlies_main_lines.csv (640B)\nStarting upload for file fiba_americup_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 539/539 [00:00<00:00, 1.61kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: fiba_americup_main_lines.csv (539B)\nStarting upload for file debug_page.html\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 173k/173k [00:00<00:00, 296kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: debug_page.html (173KB)\nStarting upload for file brazil_paulista_fpb_u20_detailed_odds.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8.29k/8.29k [00:00<00:00, 24.9kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: brazil_paulista_fpb_u20_detailed_odds.csv (8KB)\nStarting upload for file fiba_eurobasket_main_lines.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2.29k/2.29k [00:00<00:00, 6.97kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: fiba_eurobasket_main_lines.csv (2KB)\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:55:20,825 - INFO - Selenium driver closed.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Data Pipeline Execution Finished ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import time\n\nprint(\"This message prints immediately.\")\n\n# Pause execution for 3 seconds\ntime.sleep(3)\n\nprint(\"This message prints after a 3-second delay.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:56:52.861546Z","iopub.execute_input":"2025-08-25T05:56:52.862515Z","iopub.status.idle":"2025-08-25T05:56:55.868161Z","shell.execute_reply.started":"2025-08-25T05:56:52.862477Z","shell.execute_reply":"2025-08-25T05:56:55.867238Z"}},"outputs":[{"name":"stdout","text":"This message prints immediately.\nThis message prints after a 3-second delay.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# STEP 1: KAGGLE AUTH & PYTHON DEPENDENCIES\n# ==============================================================================\nprint(\"--- Installing Python Dependencies ---\")\n!pip install -q selenium pandas kaggle undetected-chromedriver\n\nimport os\nimport pandas as pd\nimport logging\nimport json\nimport re\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nfrom importlib import reload\n\n# Force logging to be active so we see all messages\nreload(logging)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nprint(\"\\n--- Setting up Kaggle API Authentication ---\")\napi = None\ntry:\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"KAGGLE_JSON\")\n    kaggle_dir = os.path.expanduser('~/.kaggle')\n    os.makedirs(kaggle_dir, exist_ok=True)\n    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n    with open(kaggle_json_path, 'w') as f: f.write(secret_value)\n    os.chmod(kaggle_json_path, 600)\n    \n    from kaggle.api.kaggle_api_extended import KaggleApi\n    api = KaggleApi()\n    api.authenticate()\n    print(\"Kaggle API Authentication Successful.\")\nexcept Exception as e:\n    logging.critical(f\"FATAL: A critical error occurred during Kaggle setup. Error: {e}\")\n    raise\n\n# ==============================================================================\n# STEP 2: SYSTEM INSTALLATIONS (CHROME) - THIS IS INTENTIONALLY LEFT BLANK\n# We will use the Chrome version pre-installed in the Kaggle environment\n# to avoid conflicts with undetected-chromedriver.\n# ==============================================================================\nprint(\"\\n--- Skipping manual Chrome installation to use the environment's default ---\")\n\n\n# ==============================================================================\n# STEP 3: SCRAPER FUNCTIONS (WITH ROBUSTNESS FIXES)\n# ==============================================================================\nimport time\nimport undetected_chromedriver as uc\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException\n\ndef get_all_leagues_and_games(driver):\n    \"\"\"\n    Scrapes the main basketball page with robust waits and debugging.\n    \"\"\"\n    url = \"https://www.pinnacle.com/en/basketball/matchups/\"\n    logging.info(f\"Navigating to matchups page: {url}\")\n    driver.get(url)\n\n    # Handle cookie banner if it appears\n    try:\n        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))).click()\n        logging.info(\"Clicked the Accept button for cookies.\"); time.sleep(2)\n    except TimeoutException:\n        logging.warning(\"Cookie banner not found or already handled.\")\n\n    leagues_data = {}\n    current_league_name = None\n\n    try:\n        # Wait for the main content container to be present.\n        content_container_selector = (By.CSS_SELECTOR, \".contentBlock.square\")\n        logging.info(\"Waiting for the main content container to load...\")\n        WebDriverWait(driver, 30).until(\n            EC.presence_of_element_located(content_container_selector)\n        )\n        logging.info(\"Main content container found. Proceeding to scrape rows.\")\n        \n        time.sleep(2) # Give JS a moment to render after container is found\n\n        all_rows = driver.find_elements(By.CSS_SELECTOR, \".contentBlock.square > div[class*='row-']\")\n        if not all_rows:\n            logging.error(\"Content container was found, but it contains no game or league rows.\")\n            with open(\"debug_page_no_rows.html\", \"w\", encoding=\"utf-8\") as f:\n                f.write(driver.page_source)\n            logging.info(\"Saved debug_page_no_rows.html to output for analysis.\")\n            return {}\n\n        logging.info(f\"Found {len(all_rows)} total rows to process on the matchups page.\")\n\n        for row in all_rows:\n            row_class = row.get_attribute('class')\n            \n            if 'row-CTcjEjV6yK' in row_class:\n                try:\n                    league_name = row.find_element(By.CSS_SELECTOR, \"a span\").text.strip()\n                    if league_name:\n                        current_league_name = league_name\n                        leagues_data[current_league_name] = []\n                        logging.info(f\"Discovered new league section: {current_league_name}\")\n                except NoSuchElementException:\n                    continue \n\n            elif 'row-k9ktBvvTsJ' in row_class and current_league_name:\n                try:\n                    game = {}\n                    link_tag = row.find_element(By.CSS_SELECTOR, \"a[href*='/basketball/']\")\n                    teams = link_tag.find_elements(By.CSS_SELECTOR, \"span.ellipsis.gameInfoLabel-EDDYv5xEfd\")\n                    game['team1'], game['team2'] = teams[0].text, teams[1].text\n                    game['game_link'] = link_tag.get_attribute('href')\n                    \n                    odds_groups = row.find_elements(By.CSS_SELECTOR, \"div.buttons-j19Jlcwsi9\")\n                    def get_text(elements, index): return elements[index].text if index < len(elements) else 'N/A'\n                    \n                    h_spans = odds_groups[0].find_elements(By.CSS_SELECTOR, \"button span\")\n                    ml_spans = odds_groups[1].find_elements(By.CSS_SELECTOR, \"span.price-r5BU0ynJha\")\n                    t_spans = odds_groups[2].find_elements(By.CSS_SELECTOR, \"button span\")\n                    \n                    game.update({'team1_moneyline': get_text(ml_spans, 0), 'team2_moneyline': get_text(ml_spans, 1),'team1_spread': get_text(h_spans, 0), 'team1_spread_odds': get_text(h_spans, 1),'team2_spread': get_text(h_spans, 2), 'team2_spread_odds': get_text(h_spans, 3),'over_total': get_text(t_spans, 0), 'over_total_odds': get_text(t_spans, 1),'under_total': get_text(t_spans, 2), 'under_total_odds': get_text(t_spans, 3)})\n                    \n                    leagues_data[current_league_name].append(game)\n                except (NoSuchElementException, IndexError):\n                    continue\n\n    except TimeoutException:\n        logging.error(\"FATAL: Timed out waiting for the main content container. The page may be blocked or changed.\")\n        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n            f.write(driver.page_source)\n        logging.info(\"Saved debug_page.html to output. This file will show what the scraper saw (e.g., a CAPTCHA).\")\n    \n    return leagues_data\n\ndef scrape_detailed_game_odds(driver, game_url):\n    logging.info(f\"Scraping detailed odds from: {game_url}\")\n    driver.get(game_url)\n    all_markets_data = []\n    try:\n        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.marketGroups-HjCkfKkLNt\"))); time.sleep(2)\n        market_groups = driver.find_elements(By.CSS_SELECTOR, \"div.marketGroup-wMlWprW2iC\")\n        for group in market_groups:\n            market_title = group.find_element(By.CSS_SELECTOR, \"span.titleText-BgvECQYfHf\").text\n            if not group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id]\"):\n                for btn in group.find_elements(By.CSS_SELECTOR, \"button\"):\n                    parts = btn.text.split('\\n')\n                    if len(parts) == 2: all_markets_data.append({'Market': market_title, 'Selection': parts[0], 'Odds': parts[1]})\n                continue\n            headers = [h.text for h in group.find_elements(By.CSS_SELECTOR, \"ul[data-test-id] > li\")]\n            button_rows = group.find_elements(By.CSS_SELECTOR, \".buttonRow-zWMLOGu5YB\")\n            for row in button_rows:\n                buttons = row.find_elements(By.TAG_NAME, 'button')\n                if len(buttons) == len(headers):\n                    for i, btn in enumerate(buttons):\n                        parts = btn.text.split('\\n')\n                        if len(parts) == 2:\n                            selection_name = f\"{headers[i]} {parts[0]}\"\n                            all_markets_data.append({'Market': market_title, 'Selection': selection_name, 'Odds': parts[1]})\n    except TimeoutException:\n        logging.error(f\"Could not load market data for URL: {game_url}\")\n    return pd.DataFrame(all_markets_data)\n\ndef to_slug(name):\n    return re.sub(r'[^a-z0-9]+', '_', name.lower()).strip('_')\n\n# ==============================================================================\n# STEP 4: MAIN DATA PIPELINE EXECUTION\n# ==============================================================================\nprint(\"\\n--- Starting Data Pipeline Execution ---\")\nif __name__ == \"__main__\" and api:\n    DATASET_SLUG = \"zachht/wnba-odds-history\" \n    WORKING_DIR = \"/kaggle/working\"\n    \n    driver = None\n    leagues_updated = []\n    try:\n        # CORRECT INITIALIZATION FOR UNDETECTED CHROMEDRIVER IN KAGGLE\n        logging.info(\"Initializing Undetected ChromeDriver...\")\n        options = uc.ChromeOptions()\n        options.add_argument(\"--headless\")\n        options.add_argument(\"--no-sandbox\")\n        options.add_argument('--disable-dev-shm-usage')\n        \n        driver = uc.Chrome(options=options)\n        logging.info(\"Undetected ChromeDriver initialized successfully.\")\n        \n        all_leagues_games = get_all_leagues_and_games(driver)\n\n        if not all_leagues_games:\n            logging.warning(\"Scraping finished, but no leagues were found on the site. Check debug files if they were created.\")\n        else:\n            for league_name, new_main_lines_data in all_leagues_games.items():\n                if not new_main_lines_data:\n                    logging.info(f\"No games found for league: {league_name}. Skipping.\")\n                    continue\n\n                logging.info(f\"\\n--- Processing League: {league_name} ({len(new_main_lines_data)} games found) ---\")\n                leagues_updated.append(league_name)\n                league_slug = to_slug(league_name)\n\n                MAIN_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_main_lines.csv\")\n                DETAILED_CSV_PATH = os.path.join(WORKING_DIR, f\"{league_slug}_detailed_odds.csv\")\n\n                try:\n                    logging.info(f\"Downloading existing files for {league_name}...\")\n                    api.dataset_download_file(DATASET_SLUG, file_name=os.path.basename(MAIN_CSV_PATH), path=WORKING_DIR)\n                    api.dataset_download_file(DATASET_SLUG, file_name=os.path.basename(DETAILED_CSV_PATH), path=WORKING_DIR)\n                    old_main_df = pd.read_csv(MAIN_CSV_PATH)\n                    old_detailed_df = pd.read_csv(DETAILED_CSV_PATH)\n                    logging.info(\"Successfully loaded existing data.\")\n                except Exception:\n                    logging.warning(f\"Could not load existing data for {league_name}. Starting with fresh history files.\")\n                    old_main_df, old_detailed_df = pd.DataFrame(), pd.DataFrame()\n\n                scrape_timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n                new_main_df = pd.DataFrame(new_main_lines_data)\n                new_main_df['timestamp'] = scrape_timestamp\n                combined_main_df = pd.concat([old_main_df, new_main_df], ignore_index=True)\n                \n                all_detailed_dfs = []\n                for game in new_main_lines_data:\n                    detailed_df = scrape_detailed_game_odds(driver, game['game_link'])\n                    if not detailed_df.empty:\n                        detailed_df['matchup'] = f\"{game['team1']} vs {game['team2']}\"\n                        all_detailed_dfs.append(detailed_df)\n                \n                if all_detailed_dfs:\n                    new_detailed_df = pd.concat(all_detailed_dfs, ignore_index=True)\n                    new_detailed_df['timestamp'] = scrape_timestamp\n                    combined_detailed_df = pd.concat([old_detailed_df, new_detailed_df], ignore_index=True)\n                    \n                    logging.info(f\"Saving combined data to local CSVs for {league_name}...\")\n                    combined_main_df.to_csv(MAIN_CSV_PATH, index=False)\n                    combined_detailed_df.to_csv(DETAILED_CSV_PATH, index=False)\n            \n            if leagues_updated:\n                logging.info(\"\\n--- Finalizing and Uploading to Kaggle ---\")\n                metadata_path = os.path.join(WORKING_DIR, 'dataset-metadata.json')\n                metadata = {\"title\": \"Pinnacle Basketball Odds History\", \"id\": DATASET_SLUG, \"licenses\": [{\"name\": \"CC0-1.0\"}]}\n                with open(metadata_path, 'w') as f: json.dump(metadata, f)\n                \n                version_note = f\"Automated odds update. Leagues updated: {', '.join(leagues_updated)}.\"\n                logging.info(f\"Pushing new dataset version. {version_note}\")\n                api.dataset_create_version(folder=WORKING_DIR, version_notes=version_note, quiet=False, dir_mode='zip')\n            else:\n                logging.warning(\"No games were found for any leagues. No new version will be pushed.\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during the main pipeline: {e}\", exc_info=True)\n    finally:\n        if driver: driver.quit(); logging.info(\"Selenium driver closed.\")\n\nprint(\"\\n--- Data Pipeline Execution Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:57:00.716461Z","iopub.execute_input":"2025-08-25T05:57:00.717315Z"}},"outputs":[{"name":"stdout","text":"--- Installing Python Dependencies ---\n\n--- Setting up Kaggle API Authentication ---\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:57:05,263 - INFO - Initializing Undetected ChromeDriver...\n","output_type":"stream"},{"name":"stdout","text":"Kaggle API Authentication Successful.\n\n--- Skipping manual Chrome installation to use the environment's default ---\n\n--- Starting Data Pipeline Execution ---\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 05:57:06,520 - INFO - patching driver executable /root/.local/share/undetected_chromedriver/undetected_chromedriver\n","output_type":"stream"}],"execution_count":null}]}